import numpy as np
import sys, os, shutil, time, subprocess, glob, re
from tqdm import tqdm
import matplotlib.pyplot as pl
from optparse import OptionParser
from annoy import AnnoyIndex
from attrdict import AttrDict
from importlib import reload
import utility 
reload(utility)

PROJ_DIR = '/cluster/work/grlab/projects/projecs2019-string-embedding/synthetic'

def load_paths(file_name):
    data_path = PROJ_DIR + '/data/' + file_name + '.npz'
    result_path = PROJ_DIR + '/results/' + file_name +'/'  
    index_path = result_path + 'index/'
    search_path = result_path + 'search/'
    eval_path = result_path + 'eval/'
    npz_path = result_path + 'npz/'
    log_path = result_path + 'log/'

    return data_path, result_path, index_path, search_path, eval_path, npz_path, log_path

def load_files(file_name, clean):
    data_path, result_path, index_path, search_path, eval_path, npz_path, log_path = load_paths(file_name)
    paths = (index_path, search_path, npz_path, eval_path, log_path)

    if clean==True:
        if os.path.exists(result_path):
            shutil.rmtree(result_path)
        os.makedirs(result_path)
        return

    for p in paths:
        if not os.path.exists(p):
            os.makedirs(p)

    Res = np.load(data_path)
    seqs = Res['seqs']
    vals = Res['vals']
    options = Res['options']
    Op = (options[()])
    if int(file_name[4:])>=7:
        gene_lens = Res['gene_lens']
        num_seqs = Res['num_seqs']
    else:
        num_seqs = [Op.num_seq]*Op.repeat
    print('num seqs = ', len(seqs), ' mean lenth of seqs = ', np.mean([len(seqs[i]) for i in range(len(seqs))]))
    print(Op)
    
    return seqs, vals, num_seqs, options, Op
    

def get_job_path(job, job_paths):
    if isinstance(job_paths, str): # check if it's only one path
        path = job_paths
    else:
        path = job_paths[job[0]]
    S = path + job[0]
    for i in range(1,len(job)):
        S = S + '_' + str(job[i])
    return S + '.done'


# generate job-specific arguments
def gen_job_args(job):
    S = ' --target ' + job[0] 
    argnames = ['', ' --seq-id ', ' --search-seq-id '] 
    for i in range(1,len(job)):
        S = S + argnames[i] + str(job[i])
    return S 



# generate the full bsub command based on job and resources 
# resources default to options defaults if not provided
def get_bsub_cmd(job, options, mem = -1, t = -1):
    _, _, _, _, _, _, log_path = load_paths(file_name)
    # if not override, use options defaults
    if mem==-1:
        mem = options.memory
    if t==-1:
        t = options.time

    # create bsub command 
    command = 'bsub -o  ' + log_path                                # save lsf.o output in the log
    command = command + ' -R "rusage[mem=' + str(mem) + '000]" '    # allocate memory 
    command = command + ' -W ' + str(t) +  ':00 '                   # allocate time 
    command = command + ' python main.py '                # name of the process

    # add options, but remove args irrelevant to worker processes
    # target, target_forward, memory, and time are only for dispatcher
    # seq_id and search_seq_id, and target are generated by dispatcher
    for k,v in options.__dict__.items():
        if k not in ['target', 'seq_id', 'search_seq_id', 'target', 'forward_target', 'memory', 'time']:
            command = command + ' --' + k.replace('_','-') + ' ' + str(v)       # get argname from varname 
    command = command + ' --time ' + str(t) + ' --memory ' + str(mem)

    # add job args to bsub and option args
    command = command + gen_job_args(job)      

    return command


# recursively compute jobs that need to run based on dependency map
def crawl_dep_recurse(Map, Set, curr):
    if curr not in Set:
        Set.add(curr)
        if curr in Map.keys():
            for job in Map[curr]:
                crawl_dep_recurse(Map, Set, job)

def crawl_dep(Map, curr):
    Set = set()
    crawl_dep_recurse(Map, Set,curr)
    return Set

def add_job(Map, job, deps):
    Map[job] = deps

def run_jobs(jobs_flat, job_dep, options, job_paths):
    _, _, _, _, _, _, log_path = load_paths(file_name)
    started = dict()
    for j in jobs_flat:
        started[j] = False
    printed = dict()
    for j in jobs_flat:
        printed[j] = False
    fcommands = open(log_path + 'commands.sh', 'a+')
    flog = open(log_path + 'log.txt', 'a+')
    final_job_done = False
    counter = -1 
    while not final_job_done:
        counter = (counter + 1) % 10
        # handled killed jobs every 10 rounds to avoid clogging the runtime
        if counter == 0:
            killed_jobs = find_killed_jobs(log_path, flog)
            for kj in killed_jobs:
                if kj.job in jobs_flat:
                    bsub_cmd = newcmd_for_killed(fname = kj.fname, job = kj.job, 
                            memory = kj.memory, t = kj.time, reason = kj.reason, 
                            log_path = log_path, options = options)
                    bsub_out = subprocess.check_output(bsub_cmd, shell=True)
                    print('RUNNING after killed : ', bsub_cmd)
                    print('RUNNING after killed : ', bsub_cmd, file = flog)
                    started[kj.job] = True
                    print(bsub_out, file = flog)
                    print(bsub_cmd, file = fcommands)
        for job in jobs_flat:
            jp = get_job_path(job,job_paths)
            if not os.path.exists(jp) and not started[job]:
                deps_satisfied = [os.path.exists(get_job_path(j,job_paths)) for j in job_dep[job] ] 
                if all(deps_satisfied):
                    bsub_cmd = get_bsub_cmd(job, options)  
                    bsub_out = subprocess.check_output(bsub_cmd, shell=True)
                    print('RUNNING: ', bsub_cmd)
                    print('RUNNING: ', bsub_cmd, file = flog)
                    started[job] = True
                    print(bsub_out, file = flog)
                    print(bsub_cmd, file = fcommands)
                    # if it's the final job print the results on the output
                    if job[0]=='final':
                        final_job_done = True
                        with open(log_path + 'final.result', 'r') as results:
                            print(results.read())
            ep = jp.replace('.done','.txt')
            if job[0]=='eval' and not printed[job] and os.path.exists(ep):
                ef = open(ep,'r')
                print(ef.read())
                printed[job] = True

        time.sleep(1)
    fcommands.close()
    flog.close()
    

# construct job tupple from input and return job
# if the job is inconsistent with job format return error=True
def get_job_from_args(target, i, j):
    if i>=0 and j>=0:
        job =  (target, i, j)
    elif i>=0:
        job =  (target, i)
    else:
        job =  (target,)
    error = False
    if target=='':
        error = True
    if target in ['search', 'eval'] and j==-1:
        error = True
    if target in ['build', 'search', 'eval'] and i==-1:
        error = True
    
    return job, error


# create new bsub command for the killed job 
def newcmd_for_killed(fname, job, memory, t, reason, log_path, options):
    if reason=='memory':
        cmd = get_bsub_cmd(job = job, options = options, mem = memory*2, t = t ) 
    elif reason=='time':
        if t<=4:
            t2 = 24
        elif t<=24:
            t2 = 120
        else:
            raise(Exception('the killed jobs has taken more than 120 hours, can\'t create the new job'))
        cmd = get_bsub_cmd(job = job, options = options, mem = memory, t = t2 ) 
    else: 
        raise(Exception('uknown reason for the killed job, can\'t create the new job'))

    return cmd
    

# get killed job from lsf.o* files in the log path and moves them to log_path/killed
# throws an exception if an error happens in the job recovery
def find_killed_jobs(log_path, flog):
    if not os.path.exists(log_path + 'killed'):
        os.makedirs(log_path + 'killed')
    # search for files with patern lsf* or *.out
    files = glob.glob(log_path + 'lsf*')
    files.extend(glob.glob(log_path + '*.out'))
    killed_jobs = []
    error = False
    e_message = ''
    for fpath in files:
        fname = fpath.split('/')[-1]
        with open(fpath, 'r') as f:
            content = f.read()
            if 'job killed' in content:
                L = content.split()
                arg_vals = ['', 10, 4, -1, -1]
                arg_names = ['--'+s for s in ['target', 'memory', 'time', 'seq-id', 'search-seq-id'] ]
                __arg_names = ['-'+s for s in ['t', 'M', 'T', 'i', 'j'] ]
                for argi,arg in enumerate(arg_names):
                    if arg in L:
                        arg_vals[argi] = L[L.index(arg)+1]
                    elif __arg_names[argi] in L:
                        arg_vals[argi] = L[L.index(__arg_names[argi]) + 1]
                    if argi>0 and isinstance(arg_vals[argi], str):
                        s = re.sub(r'\W+', '', arg_vals[argi])
                        arg_vals[argi] = int(s)
                target, memory, time, i, j = arg_vals
                reason = 'unkown'
                if 'TERM_MEMLIMIT' in content:
                    reason = 'memory'
                elif 'TERM_RUNLIMIT' in content:
                    reason = 'time' 
                elif 'TERM_OWNER' in content:
                    reason = 'user'
                job, e =  get_job_from_args(target, i, j)
                if e==True or reason=='unkonwn':
                    error = True
                if e==True:
                    e_message = ("killed job recovered from LSF file incorrectly: "+ str(job))
                elif reason == 'unkown':
                    e_message = ("job killed for unkown reason "+ str(job) )
                Dict = {'fname' : fname, 'job' : job, 'memory':memory, 'time' : time, 'reason': reason}
                kj = AttrDict(Dict)
                shutil.move( fpath, log_path + 'killed/' + fname)
                print('found killed job : ', job, ' mem = ', memory, ' time = ', time, ', and moved lsf file: ', fname, ' to killed/ ' )
                print('found killed job : ', job, ' mem = ', memory, ' time = ', time, ', and moved lsf file: ', fname, ' to killed/ ' , file = flog)
                # add job if it's not killed by the user
                if reason != 'user':
                    killed_jobs.append(kj)
    # raise an exception of an error occured in processing killed jobs
    if error == True:
        raise(Exception(e_message)) 

    return killed_jobs



if __name__ == '__main__':
    parser = OptionParser()
    parser.add_option('-f','--file-name',dest='file_name', default = 'seqs0', type='string', help = 'seq file to load')
    parser.add_option('-i','--seq-id',dest='seq_id', default = 0, type='int', help = 'seq id to build index for')
    parser.add_option('-j','--search-seq-id',dest='search_seq_id', default = 0, type='int', help = 'search sequence id')
    parser.add_option('-k','--small-k',dest='small_k', default = 3, type='int', help = 'small k to do kmer counting')
    parser.add_option('-K','--big-k',dest='big_k', default = 100, type='int', help = 'big K to slide over the strings')
    parser.add_option('-D','--proj-dim',dest='proj_dim', default = 120, type='int', help = 'number of dimensions to project onto')
    parser.add_option('-n','--num-trees',dest='num_trees', default = 2, type='int', help = 'number of trees to be built')
    parser.add_option('-s','--step-build',dest='step_build', default = 5, type='int', help = 'step for indices of KNN built')
    parser.add_option('-m','--metric',dest='metric', default = 'euclidean', type='string', help = 'metric to be used')
    parser.add_option('-S','--step-search',dest='step_search', default = 5, type='int', help = 'step for indices of KNN built')
    parser.add_option('-N','--num-neighbors',dest='num_neighbors', default = 50, type='int', help = 'number of nearest neighbors to search')
    parser.add_option('-t','--target',dest='target', default = 'build', type='string', help = 'metric to be used')
    parser.add_option('-F','--forward-target',dest='forward_target', default = 'merge', type='string', help = 'metric to be used')
    parser.add_option('-M','--memory',dest='memory', default = 30, type='int', help = 'memory to allocate for each process')
    parser.add_option('-T','--time',dest='time', default = 4, type='int', help = 'designated hours for each process ')
    (options, args) = parser.parse_args()

    sid = options.seq_id
    sid2 = options.search_seq_id
    proj_dim = options.proj_dim
    k_small = options.small_k 
    k_big = options.big_k
    num_trees = options.num_trees
    step_build = options.step_build
    step_search = options.step_search
    num_neighbors = options.num_neighbors
    metric = options.metric
    file_name = options.file_name

    data_path, result_path, index_path, search_path, eval_path, npz_path, log_path = load_paths(file_name)
    job_paths = {'clean':result_path, 
            'build': index_path,
            'search' : search_path,
            'eval': eval_path,
            'merge': result_path ,
            'final': result_path }


    if options.target=='all':
        summary = np.load(result_path+'num.npz')
        N = summary['N']
        jobs_dep = dict()
        add_job(jobs_dep,('clean',),[])
        build_jobs = []
        for i in range(N):
            job = ('build',i) 
            add_job(jobs_dep,job,[('clean',)])
            build_jobs.append(job)
        search_jobs = []
        for i in range(N):
            for j in range(N):
                if i!=j:
                    job = ('search',i,j)
                    add_job( jobs_dep,job,[('build',i),('build',j)]  )
                    search_jobs.append(search_jobs)
        eval_jobs = []
        for i in range(N):
            for j in range(N):
                if i!=j:
                    job = ('eval',i,j)
                    add_job( jobs_dep, job, [ ('search',i,j) ]  )
                    eval_jobs.append(job)
        add_job(jobs_dep, ('merge',), eval_jobs)

        dep_job = {'final':[], 
                'clean': [('clean',)],
                'build' : build_jobs,
                'search': search_jobs,
                'eval': eval_jobs, 
                'merge': [('merge',)] }
        add_job(jobs_dep,('final',), dep_job[options.forward_target]) 
        jobs_flat = crawl_dep(jobs_dep, ('final',))
        run_jobs(jobs_flat, jobs_dep, options, job_paths) 

    elif options.target=='final':
        with open(log_path + 'final.result', 'r') as results:
            print(results.read())
        job_done = ('final', )

        
    elif options.target=='merge':
        quads = set()
        summary = np.load(result_path+'num.npz')
        N = summary['N']
        num_seqs = summary['num_seqs']
        Op = summary['Op']
        Op = Op[()]
        total_count = 0
        total_correct = 0
        for i in range(N):
            for j in range(N):
                if i!=j:
                    res = np.load(eval_path +str(i)+'_'+str(j)+'.npz') 
                    q = res['quadrupples']
                    quads = quads.union(q[()])
                    total_count = total_count + res['total_count']
                    total_correct = total_correct + res['total_correct']
        Total = 0
        for ns in num_seqs:
            Total = Total + ns*(ns-1)/2 * Op.num_genes
        if 'seq_lens' in summary.files:
            seq_lens = summary['seq_lens']
        else:
            seqs, _, _, _, _ = load_files(file_name, clean=False)
            seq_lens = [len(seqs[i]) for i in range(len(seqs))]

        fresult = open(log_path + 'final.result', 'w+')
        print('FINAL RESULT ' + '#'*50, file=fresult)
        print('number of seqs: ', N , file=fresult)
        print('seq options : ', Op, file = fresult)
        print('mean seq len: ', np.mean(seq_lens), file = fresult)
        print('false positive : ', (total_count - total_correct ) *1.0/total_count, file = fresult)
        print('final recall : ', len(quads)*1.0/Total, file=fresult)
        print('#'*50, file = fresult)
        fresult.close()

        job_done = ('merge', )

    elif options.target=='clean':
        load_files(options.file_name, clean=True)
        seqs, vals, num_seqs, opts, Op = load_files(file_name, clean=False)
        seq_lens = [len(seqs[i]) for i in range(len(seqs))]
        np.savez(result_path+'num.npz', N=len(seqs), num_seqs = num_seqs, options = options, Op = Op, seq_lens = seq_lens)

        job_done = ('clean', )

    elif options.target=='build':
    
        seqs, vals, num_seqs, opts, Op  = load_files(file_name, clean=False)
        kmers, s_kmer_vals, kmer_pos, kmer_seq_id = utility.list_kmers_simple([seqs[sid]], vals = [vals[sid]],  
                                                 k = k_small, addy = True, padding = int(k_big))
        kmer_vals = utility.get_kmver_vals(s_kmer_vals, k_big)
        build_indices = np.arange(0,len(kmers),step_build)

        convolved = utility.random_projection(kmers, k_big, proj_dim)
        np.savez(npz_path+'data'+str(sid)+'.npz', 
                 convolved=convolved, 
                 num_seqs = num_seqs,
                 kmers = kmers, 
                 kmer_pos = kmer_pos,
                 kmer_vals = kmer_vals, 
                 build_indices = build_indices)

        knn_index = utility.build_index(matrix = convolved, 
                                        indices = build_indices, 
                                        num_trees = num_trees, 
                                        metric = metric, 
                                        index_path = index_path + str(sid) + '.ann')
        job_done = ('build', sid)

    elif options.target=='search':

        data1 = np.load(npz_path+'data'+str(sid)+'.npz')
        knn_index = AnnoyIndex(proj_dim, metric)
        knn_index.load(index_path + str(sid) + '.ann', prefault=True)
        build_indices = data1['build_indices']

        data2 = np.load(npz_path+'data'+str(sid2)+'.npz')
        convolved = data2['convolved']
        search_indices = np.arange(0,convolved.shape[0],step_search)
        NN, NN_dist = utility.knn_search_value(knn_index, convolved, search_indices, build_indices,num_neighbors)
        np.savez(search_path + str(sid)+'_'+str(sid2) + '.npz', NN=NN, NN_dist=NN_dist, search_indices = search_indices)
        job_done = ('search', sid, sid2)

    elif options.target=='eval':
        job_done = ('eval', sid, sid2)

        data1 = np.load(npz_path+'data'+str(sid)+'.npz')
        data2 = np.load(npz_path+'data'+str(sid2)+'.npz')
        summary = np.load(result_path + 'num.npz')
        Op = summary['Op']
        Op = Op[()]
        num_seqs = data1['num_seqs']
        kvals1 = data1['kmer_vals']
        kvals2 = data2['kmer_vals']
        search_data = np.load(search_path + str(sid)+'_'+str(sid2) + '.npz')
        NN = search_data['NN']
        NN_dist = search_data['NN_dist']
        search_indices = search_data['search_indices']
        build_indices = data1['build_indices']

        total_len = len(search_indices)
        total_count = 0
        total_correct = 0
        quadrupples = []
        for kii in tqdm(range(total_len)):
            ki = search_indices[kii]
            kval2 = kvals2[ki]
            neighbor_indices = NN[kii]
            for ki2 in neighbor_indices:
                kval1 = kvals1[ki2]
                total_count = total_count + 1
                if kval1==kval2 and kval1>0:
                    total_correct = total_correct + 1
                    s1,s2 = np.sort([sid,sid2])
                    quadrupples.append( ( (s1,s2) , (kval1, kval2) ) )
        quadrupples = set(quadrupples)
        np.savez(eval_path +str(sid)+'_'+str(sid2)+'.npz', 
                quadrupples = quadrupples, 
                total_count = total_count, 
                total_correct = total_correct)

        print('false positive = ', (total_count - total_correct)/total_count)
        print('recall = ', len(quadrupples)*1.0/Op.num_genes)
        jp = get_job_path(job_done, job_paths).replace('.done','')
        feval = open(jp+'.txt', 'w+')
        print('EVAL RESULT ' + '#'*50, file=feval)
        print('eval job: -i ', sid, ', -j ', sid2, file=feval)
        print('false positive = ', (total_count - total_correct)/total_count, file=feval)
        print('recall = ', len(quadrupples)*1.0/Op.num_genes, file=feval)
        print( '#'*62, file=feval)
        

    # print done for all targets 
    # except for all target that is the dispatcher 
    if options.target!='all':
        Job_path = get_job_path(job_done, job_paths)
        print('dummy', file=open(Job_path, 'w+'))
